{"JOB DESCRIPTION": " Job DescriptionThis role is for candidates proficient in Apache Spark and Python programming. The candidate will be involved in processing data using PySpark like reading data from external sources, merge data, execute data enrichment and load them into target data destinations.Roles & ResponsibilitiesThe ideal candidate will-\u00a0-\u00a0Participate in design and development of Big Data analytical applications-\u00a0Design, support and continuously enhance the project code base, continuous integration pipeline, etc.-\u00a0Write complex ETL processes and frameworks for analytics and data management-\u00a0Implement large-scale near real-time streaming data processing pipelinesEligibility Criteria- Must have a\u00a0BE/ B. Tech degree in Computer Science or related quantitative field with 3 - 8 years of relevant experience- Must have experience in working on Hadoop distribution systems (Cloudera / Hortonworks/ MapR)- Must have experience in working on Hadoop clusters and exposure to Hadoop tools like Spark, Oozie, Sqoop, Flume, KAFKA, HUE, HBase etc.- Must have a strong proficiency in using query languages such as SQL, Hive and SparkSQL- Must have experience with version control systems like Git in particular- Must have experience in Python, PySpark and willingness to learn new languages, as and when needed- Must have extensive experience in working on RDBMS and ETL tools- Must be comfortable in analyzing complex, high-volume & high dimension data from varying sources", "ROLE": "Software Developer", "INDUSTRY TYPE": "IT-Software, Software Services", "FUNCTIONAL AREA": "IT Software - Application Programming, Maintenance", "EMPLOYMENT TYPE": "Full Time, Permanen", "ROLE CATEGORY": "Programming & Desig", "EDUCATION": {"UG": "B.Tech/B.E. in Any Specialization"}, "KEY SKILL": ["Azure", "Cloudera", "Hadoop", "Big Data", "pyspark", "Impala", "SQL", "Software Engineering", "GCP", "Cloud", "Technology Consulting", "Spark", "AWS", "Python"]}