{"JOB DESCRIPTION": "\n\nSenior Data Developer Apply \n\nDescription\n\nLooking for the big data engineer to help shape our technology and product roadmap. You will be part of the fast - paced ,  entrepreneurial team that enables Big Data and batch /  real - time analytical solutions leveraging transformational technologies (Python ,  Scala ,  Java ,  Hadoop ,  MapReduce ,  Kafka ,  Hive ,  HBase ,  Spark ,  Storm ,  etc.) to deliver innovative solutions.\n\nAs a data developer at Delhivery you are expected to work closely with Product team to do rapid development of new features  /  enhancements ,  work with the Data Services team to design and build data stores ,  write effective MapReduce  /  aggregations ,  write quality code and work with your lead and team to drive quality and efficiency.\n\nResponsibility\n\nYou will be responsible for delivering high - value next - generation products on aggressive deadlines and will be required to write high - quality ,  highly optimized / high - performance and maintainable code\n\nManage ETL / ELT pipelines of various Microservices\n\nWork on distributed / big - data system to build ,  release and maintain an Always On scalable data processing and reporting platform\n\nWork on relational and NoSQL databases\n\nBuild scalable architectures for data storage ,  transformation and analysis\n\nAbility to work in the fast - paced and dynamic environment\n\nQualifications\n\nOverall 3+ years of IT experience in a variety of industries ,  which includes hands on experience in Big Data Analytics and development\n\n2+ yrs of experience in writing pyspark for data transformation.\n\n2+ years of experience with detailed knowledge of data warehouse technical architectures ,  ETL /  ELT ,  reporting / analytic tools ,  and data security\n\n2+ years of experience in designing data warehouse solutions and integrating technical components\n\n2+ yrs of experience leading data warehousing and analytics projects ,  including using AWS technologies  -  Redshift ,  S3 ,  EC2 ,  Data - pipeline and other big data technologies\n\n1+ yr of experience of BI implementation in the Cloud.\n\nExposure in at least one ETL tool.\n\nExposure to cloud Datawarehouse will be a plus.\n\nExposure in at least one reporting tool like Redash / Tableau / similar will be a plus.\n\nFamiliarity with Linux / Unix scripting\n\nExpertise with the tools in Hadoop Ecosystem including Pig ,  Hive ,  HDFS MapReduce ,  Sqoop ,  Storm ,  Spark ,  Kafka ,  Yarn ,  Oozie ,  and Zookeeper.\n\nSolid experience building APIs (REST) ,  Java services ,  or Docker Microservices\n\nExperience with data pipelines using Apache Kafka ,  Storm ,  Spark ,  AWS Lambda or similar technologies\n\nExperience working with terabyte data sets using relational databases (RDBMS) and SQL\n\nExperience using Agile / Scrum methodologies to iterate quickly on product changes ,  developing user stories and working through backlogs.\n\nExperience with Hadoop ,  MPP DB platform ,  other NoSQL (Mongo ,  Cassandra) technologies will be a big plus\n\n", "ROLE": "Software Developer", "INDUSTRY TYPE": "Courier, Transportation, Freight , Warehousing", "FUNCTIONAL AREA": "IT Software - DBA, Datawarehousing", "EMPLOYMENT TYPE": "Full Time, Permanen", "ROLE CATEGORY": "Programming & Desig", "EDUCATION": {"UG": "B.Tech/B.E. in Computers", "PG": "M.Tech in Computers"}, "KEY SKILL": ["NoSQL", "Linux", "RDBMS", "data security", "Analytical", "Data processing", "Apache", "SQL", "Python", "HBase"]}